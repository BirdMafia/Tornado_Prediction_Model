# -*- coding: utf-8 -*-
"""MLDAK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNnLcIGcA3ZxH4laAh5DeC-S1vb6n7dG
"""

# ======================
# 1. SETUP & DATA LOADING
# ======================
from google.colab import drive
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Load the dataset
try:
    file_path = '/content/drive/My Drive/Colab Notebooks/1950-2023_actual_tornadoes.csv'
    df = pd.read_csv(file_path)
    print("✅ Dataset loaded successfully")
    print(f"Initial shape: {df.shape}")
except Exception as e:
    print(f"❌ Error loading file: {e}")
    raise

# ======================
# 2. STREAMLINED FEATURE ENGINEERING
# ======================
def clean_time_series(df):
    """Optimized feature engineering with reduced memory footprint"""
    # Convert date/time (more memory efficient)
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df['time'] = pd.to_datetime(df['time'].astype(str).str.strip(),
                              format='%H:%M:%S',
                              errors='coerce').dt.time

    # Core temporal features
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['hour'] = pd.to_datetime(df['time'].astype(str),
                              format='%H:%M:%S',
                              errors='coerce').dt.hour

    # Most impactful engineered features only
    df['day_of_year'] = df['date'].dt.dayofyear
    df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))
    df['lat_rounded'] = round(df['slat'], 1)

    # Drop intermediate columns to save memory
    df.drop(columns=['date', 'time'], inplace=True, errors='ignore')

    return df

df = clean_time_series(df)

# ======================
# 3. TARGET OPTIMIZATION
# ======================
def prepare_target(df):
    """More balanced target categories"""
    df['mag'] = pd.to_numeric(df['mag'], errors='coerce')

    # Optimized bins based on EF scale distribution
    bins = [-0.1, 0.9, 1.9, 2.9, 10]  # Fewer, more balanced categories
    labels = ['EF0-EF1', 'EF2', 'EF3+', 'Violent']

    df['mag_category'] = pd.cut(df['mag'], bins=bins, labels=labels)
    df = df.dropna(subset=['mag_category', 'slat', 'slon', 'stf'])

    # Remove smallest classes
    valid_categories = df['mag_category'].value_counts()[df['mag_category'].value_counts() > 200].index
    df = df[df['mag_category'].isin(valid_categories)]

    return df

df = prepare_target(df)

# ======================
# 4. OPTIMIZED FEATURE SELECTION
# ======================
features = [
    'year', 'month', 'hour',
    'slat', 'slon', 'stf',
    'day_of_year', 'hour_sin',
    'lat_rounded'
]
target = 'mag_category'

# Final cleanup
df = df.dropna(subset=features + [target])

# ======================
# 5. RESOURCE-EFFICIENT MODEL TRAINING
# ======================
# Simplified preprocessing
numeric_features = features  # All features are numeric now
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features)
    ])

# Reduced parameter grid for faster tuning
param_grid = {
    'classifier__n_estimators': [150, 200],
    'classifier__max_depth': [15, 20],
    'classifier__min_samples_split': [10],
    'classifier__max_features': ['sqrt']
}

# Optimized pipeline
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(
        class_weight='balanced',
        random_state=42,
        n_jobs=1  # Reduced to prevent worker issues
    ))
])

# More efficient grid search
grid_search = GridSearchCV(
    model,
    param_grid,
    cv=3,
    scoring='accuracy',
    verbose=2,
    n_jobs=1  # Single job to prevent worker crashes
)

# Train model
try:
    grid_search.fit(df[features], df[target])
    print("\n✅ Model trained successfully")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best CV accuracy: {grid_search.best_score_:.2%}")
except Exception as e:
    print(f"\n❌ Model training failed: {e}")
    raise

# ======================
# 6. EVALUATION
# ======================
best_model = grid_search.best_estimator_
X_train, X_test, y_train, y_test = train_test_split(
    df[features], df[target],
    test_size=0.2,
    random_state=42,
    stratify=df[target]
)

y_pred = best_model.predict(X_test)

print("\nModel Performance:")
print(classification_report(y_test, y_pred))
print(f"\nExact Accuracy: {accuracy_score(y_test, y_pred):.2%}")

# Feature importance
importances = best_model.named_steps['classifier'].feature_importances_
plt.figure(figsize=(10, 6))
plt.barh(features, importances)
plt.title("Feature Importance")
plt.show()

# ======================
# 7. SAMPLE PREDICTION
# ======================
try:
    sample_data = pd.DataFrame([{
        'year': 2023,
        'month': 5,
        'hour': 18,
        'slat': 35.5,
        'slon': -97.5,
        'stf': 40,
        'day_of_year': 135,
        'hour_sin': np.sin(18 * (2 * np.pi / 24)),
        'lat_rounded': 35.5
    }])

    prediction = best_model.predict(sample_data)
    print(f"\nSample Prediction: {prediction[0]}")
except Exception as e:
    print(f"\nPrediction error: {e}")

# ======================
# 8. FINAL SUMMARY
# ======================
print("\nFinal dataset summary:")
print(f"Total records: {len(df)}")
print("Target distribution:")
print(df[target].value_counts(normalize=True).apply(lambda x: f"{x:.1%}"))